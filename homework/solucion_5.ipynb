{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Cargar y limpiar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1 Cargar datos de entrenamiento\n",
    "import pandas as pd  #  type: ignore\n",
    "\n",
    "data_train = pd.read_csv(\"../files/input/train_data.csv.zip\", index_col=False, compression=\"zip\")\n",
    "data_test = pd.read_csv(\"../files/input/test_data.csv.zip\", index_col=False, compression=\"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocese los datos.\n",
    "# - Cree la columna 'Age' a partir de la columna 'Year'.\n",
    "#   Asuma que el año actual es 2021.\n",
    "# - Elimine las columnas 'Year' y 'Car_Name'.\n",
    "\n",
    "data_train[\"Age\"] = 2021 - data_train[\"Year\"]\n",
    "data_test[\"Age\"] = 2021 - data_test[\"Year\"]\n",
    "\n",
    "data_train = data_train.drop(columns=[\"Year\", \"Car_Name\"])\n",
    "data_test = data_test.drop(columns=[\"Year\", \"Car_Name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 2. Divida los datasets en x_train, y_train, x_test, y_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_train.drop(columns=[\"Present_Price\"])\n",
    "y_train = data_train[\"Present_Price\"]\n",
    "\n",
    "x_test = data_test.drop(columns=[\"Present_Price\"])\n",
    "y_test = data_test[\"Present_Price\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 3: Creación del pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3.\n",
    "# Cree un pipeline para el modelo de clasificación. Este pipeline debe\n",
    "# contener las siguientes capas:\n",
    "# - Transforma las variables categoricas usando el método\n",
    "#   one-hot-encoding.\n",
    "# - Escala las variables numéricas al intervalo [0, 1].\n",
    "# - Selecciona las K mejores entradas.\n",
    "# - Ajusta un modelo de regresion lineal.\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "categorical_features = [\"Fuel_Type\", \"Selling_type\", \"Transmission\"]\n",
    "numerical_features = [col for col in x_train.columns if col not in categorical_features]\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "        (\"num\", MinMaxScaler(), numerical_features),\n",
    "    ],\n",
    "    remainder=MinMaxScaler(),\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"feature_selection\", SelectKBest(f_regression)),\n",
    "        (\"classifier\", LinearRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "# Ajustar el pipeline a los datos de entrenamiento\n",
    "pipeline.fit(x_train, y_train)\n",
    "\n",
    "# Evaluar el modelo en el conjunto de prueba\n",
    "print(\"Modelo entrenado. Precisión en test:\", pipeline.score(x_test, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 4. Definir los hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 4.\n",
    "# Optimice los hiperparametros del pipeline usando validación cruzada.\n",
    "# Use 10 splits para la validación cruzada. Use el error medio absoluto\n",
    "# para medir el desempeño modelo.\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"feature_selection__k\": range(1, 12),\n",
    "}\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, cv=10, scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
    "\n",
    "grid_search.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 5. Guardar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 5.\n",
    "# Guarde el modelo (comprimido con gzip) como \"files/models/model.pkl.gz\".\n",
    "# Recuerde que es posible guardar el modelo comprimido usanzo la libreria gzip.\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "# Guardar el modelo comprimido con gzip\n",
    "with gzip.open('../files/models/model.pkl.gz', 'wb') as file:\n",
    "    pickle.dump(grid_search, file)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paso 6. Calcular la matriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 6.\n",
    "# Calcule las metricas r2, error cuadratico medio, y error absoluto medio\n",
    "# para los conjuntos de entrenamiento y prueba. Guardelas en el archivo\n",
    "# files/output/metrics.json. Cada fila del archivo es un diccionario con\n",
    "# las metricas de un modelo. Este diccionario tiene un campo para indicar\n",
    "# si es el conjunto de entrenamiento o prueba. Por ejemplo:\n",
    "#\n",
    "# {'type': 'metrics', 'dataset': 'train', 'r2': 0.8, 'mse': 0.7, 'mad': 0.9}\n",
    "# {'type': 'metrics', 'dataset': 'test', 'r2': 0.7, 'mse': 0.6, 'mad': 0.8}\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error,median_absolute_error\n",
    "\n",
    "# Predecir en los conjuntos de entrenamiento y prueba\n",
    "# y_train_pred = grid_search.best_estimator_.predict(x_train)\n",
    "# y_test_pred = grid_search.best_estimator_.predict(x_test)\n",
    "\n",
    "# Calcular las métricas\n",
    "metrics_train = {\n",
    "    \"type\": \"metrics\",\n",
    "    \"dataset\": \"train\",\n",
    "    \"r2\": float(r2_score(y_train, grid_search.predict(x_train))),\n",
    "    \"mse\": float(mean_squared_error(y_train, grid_search.predict(x_train))),\n",
    "    \"mad\": float(median_absolute_error(y_train, grid_search.predict(x_train))),\n",
    "}\n",
    "\n",
    "metrics_test = {\n",
    "    \"type\": \"metrics\",\n",
    "    \"dataset\": \"test\",\n",
    "    \"r2\": float(r2_score(y_test, grid_search.predict(x_test))),\n",
    "    \"mse\": float(mean_squared_error(y_test, grid_search.predict(x_test))),\n",
    "    \"mad\": float(median_absolute_error(y_test, grid_search.predict(x_test))),\n",
    "}\n",
    "\n",
    "\n",
    "# Guardar las métricas en un archivo JSON\n",
    "import json\n",
    "\n",
    "    \n",
    "with open('../files/output/metrics.json', 'w') as f:\n",
    "    f.write(json.dumps(metrics_train) + \"\\n\")\n",
    "    f.write(json.dumps(metrics_test) + \"\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
